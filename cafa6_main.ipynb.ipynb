{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pnuKNjdLA_EW",
        "outputId": "84f7f39c-ed62-4f10-ecec-fcbf0b168aae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DKpyiPAxBGLF"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# CAFA6 + ESM2 - PHIÃŠN Báº¢N Tá»I Æ¯U Tá»C Äá»˜ (SPEED OPTIMIZED)\n",
        "# =============================================================================\n",
        "\n",
        "# 1. MOUNT DRIVE\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 2. CONFIG & IMPORTS\n",
        "import gc\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score\n",
        "from tqdm import tqdm\n",
        "\n",
        "# --- Cáº¤U HÃŒNH ÄÆ¯á»œNG DáºªN ---\n",
        "BASE_DIR = \"/content/drive/MyDrive/CAFA6\"\n",
        "\n",
        "TRAIN_SEQ_PATH   = os.path.join(BASE_DIR, \"Train/train_sequences.fasta\")\n",
        "TRAIN_TERMS_PATH = os.path.join(BASE_DIR, \"Train/train_terms.tsv\")\n",
        "TEST_SEQ_PATH    = os.path.join(BASE_DIR, \"Test/testsuperset.fasta\")\n",
        "SAMPLE_SUB_PATH  = os.path.join(BASE_DIR, \"sample_submission.tsv\")\n",
        "\n",
        "EMBED_TRAIN_PATH = os.path.join(BASE_DIR, \"train_embeddings_esm2_t6.npy\")\n",
        "EMBED_TEST_PATH  = os.path.join(BASE_DIR, \"test_embeddings_esm2_t6.npy\")\n",
        "\n",
        "# --- [Tá»I Æ¯U]  ---\n",
        "EMBED_BATCH_SIZE = 64\n",
        "BATCH_SIZE_TRAIN = 256\n",
        "HIDDEN_DIM = 512\n",
        "LR = 1e-3\n",
        "EPOCHS = 10\n",
        "MIN_GO_FREQ = 50\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Device: {device}\")\n",
        "\n",
        "# CÃ i thÆ° viá»‡n ESM\n",
        "try:\n",
        "    import esm\n",
        "except ImportError:\n",
        "    import esm\n",
        "\n",
        "# 3. HÃ€M Äá»ŒC FASTA\n",
        "def read_fasta(path):\n",
        "    ids, seqs = [], []\n",
        "    with open(path, \"r\") as f:\n",
        "        current_id = None\n",
        "        current_seq = []\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if line.startswith(\">\"):\n",
        "                if current_id is not None:\n",
        "                    ids.append(current_id)\n",
        "                    seqs.append(\"\".join(current_seq))\n",
        "\n",
        "                raw = line[1:].split()[0]\n",
        "                if \"|\" in raw:\n",
        "                    parts = raw.split(\"|\")\n",
        "                    if len(parts) >= 2 and parts[0] in (\"sp\", \"tr\"):\n",
        "                        current_id = parts[1]\n",
        "                    else:\n",
        "                        current_id = raw\n",
        "                else:\n",
        "                    current_id = raw\n",
        "\n",
        "                current_seq = []\n",
        "            else:\n",
        "                current_seq.append(line)\n",
        "\n",
        "        if current_id:\n",
        "            ids.append(current_id)\n",
        "            seqs.append(\"\".join(current_seq))\n",
        "\n",
        "    return pd.DataFrame({\"EntryID\": ids, \"sequence\": seqs})\n",
        "\n",
        "# 4. LOAD MODEL ESM2\n",
        "print(\"Loading ESM2 Model...\")\n",
        "model, alphabet = esm.pretrained.esm2_t6_8M_UR50D()\n",
        "model = model.to(device)\n",
        "model.eval()\n",
        "batch_converter = alphabet.get_batch_converter()\n",
        "repr_layer = 6\n",
        "embed_dim = 320\n",
        "\n",
        "# 5. HÃ€M Táº O EMBEDDING\n",
        "def get_memmap_embedding(ids, seqs, save_path, batch_size=64):\n",
        "    n_samples = len(seqs)\n",
        "\n",
        "    # Náº¿u file embedding Ä‘Ã£ tá»“n táº¡i thÃ¬ load láº¡i\n",
        "    if os.path.exists(save_path):\n",
        "        print(f\"âœ… ÄÃ£ tÃ¬m tháº¥y file: {save_path}\")\n",
        "        print(\"   -> Äang load láº¡i tá»« á»• cá»©ng (Bá»Ž QUA cháº¡y láº¡i ESM2)...\")\n",
        "        try:\n",
        "            mmap = np.memmap(save_path, dtype='float16', mode='r',\n",
        "                             shape=(n_samples, embed_dim))\n",
        "            return mmap\n",
        "        except Exception as e:\n",
        "            print(f\"   -> File lá»—i, sáº½ cháº¡y láº¡i. Lá»—i: {e}\")\n",
        "\n",
        "    print(f\"ðŸš€ KhÃ´ng tháº¥y file cÅ©, báº¯t Ä‘áº§u cháº¡y ESM2 (Sáº½ máº¥t thá»i gian láº§n Ä‘áº§u)...\")\n",
        "    print(f\"   -> LÆ°u táº¡i: {save_path}\")\n",
        "\n",
        "    mmap_arr = np.memmap(save_path, dtype='float16', mode='w+',\n",
        "                         shape=(n_samples, embed_dim))\n",
        "\n",
        "    for i in tqdm(range(0, n_samples, batch_size), desc=\"Embedding\"):\n",
        "        batch_ids = ids[i : i + batch_size]\n",
        "        batch_seqs_raw = seqs[i : i + batch_size]\n",
        "        # Cáº¯t chuá»—i dÃ i quÃ¡ giá»›i háº¡n\n",
        "        batch_seqs = [s[:1022] for s in batch_seqs_raw]\n",
        "\n",
        "        data = list(zip(batch_ids, batch_seqs))\n",
        "        labels, strs, toks = batch_converter(data)\n",
        "        toks = toks.to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # Mixed precision Ä‘á»ƒ nhanh hÆ¡n\n",
        "            with torch.cuda.amp.autocast():\n",
        "                results = model(toks, repr_layers=[repr_layer],\n",
        "                                return_contacts=False)\n",
        "                token_reps = results[\"representations\"][repr_layer]\n",
        "                # Average pooling trÃªn chiá»u sequence (bá» [CLS], [EOS])\n",
        "                emb = token_reps[:, 1:-1].mean(1)\n",
        "\n",
        "            emb_cpu = emb.float().cpu().numpy().astype(np.float16)\n",
        "\n",
        "        current_len = len(batch_seqs)\n",
        "        mmap_arr[i : i + current_len] = emb_cpu\n",
        "\n",
        "        del toks, results, token_reps, emb, emb_cpu\n",
        "        if i % (batch_size * 10) == 0:\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    mmap_arr.flush()\n",
        "    return np.memmap(save_path, dtype='float16', mode='r',\n",
        "                     shape=(n_samples, embed_dim))\n",
        "\n",
        "# 6. Xá»¬ LÃ TRAIN\n",
        "print(\"\\n--- 1. PREPARING TRAIN DATA ---\")\n",
        "train_seq_df = read_fasta(TRAIN_SEQ_PATH)\n",
        "train_terms = pd.read_csv(TRAIN_TERMS_PATH, sep=\"\\t\")\n",
        "\n",
        "go_counts = train_terms[\"term\"].value_counts()\n",
        "valid_go = set(go_counts[go_counts >= MIN_GO_FREQ].index)\n",
        "train_terms_filt = train_terms[train_terms[\"term\"].isin(valid_go)].copy()\n",
        "\n",
        "entry_to_terms = (\n",
        "    train_terms_filt.groupby(\"EntryID\")[\"term\"]\n",
        "    .apply(list)\n",
        "    .reset_index()\n",
        "    .rename(columns={\"term\": \"terms\"})\n",
        ")\n",
        "\n",
        "train_df = train_seq_df.merge(entry_to_terms, on=\"EntryID\", how=\"inner\")\n",
        "print(f\"Sá»‘ lÆ°á»£ng máº«u train: {len(train_df)}\")\n",
        "\n",
        "# Cháº¡y Embedding Train\n",
        "X_train_mmap = get_memmap_embedding(\n",
        "    train_df[\"EntryID\"].tolist(),\n",
        "    train_df[\"sequence\"].tolist(),\n",
        "    EMBED_TRAIN_PATH,\n",
        "    batch_size=EMBED_BATCH_SIZE,\n",
        ")\n",
        "\n",
        "print(\"Encoding Labels...\")\n",
        "mlb = MultiLabelBinarizer(sparse_output=True)\n",
        "Y_sparse = mlb.fit_transform(train_df[\"terms\"])\n",
        "Y_dense = Y_sparse.toarray().astype(np.float32)\n",
        "n_labels = len(mlb.classes_)\n",
        "\n",
        "idx_train, idx_val = train_test_split(\n",
        "    np.arange(len(train_df)),\n",
        "    test_size=0.1,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "del train_seq_df, train_terms, train_terms_filt, entry_to_terms, Y_sparse\n",
        "gc.collect()\n",
        "\n",
        "# 7. MODEL & DATASET\n",
        "class MemmapDataset(Dataset):\n",
        "    def __init__(self, memmap_array, labels, indices):\n",
        "        self.memmap_array = memmap_array\n",
        "        self.labels = labels\n",
        "        self.indices = indices\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.indices)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        real_idx = self.indices[idx]\n",
        "        return (\n",
        "            torch.tensor(self.memmap_array[real_idx], dtype=torch.float32),\n",
        "            torch.tensor(self.labels[real_idx], dtype=torch.float32),\n",
        "        )\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, in_dim, hidden_dim, out_dim):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(in_dim, hidden_dim),\n",
        "            nn.BatchNorm1d(hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.4),\n",
        "            nn.Linear(hidden_dim, out_dim),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "# 8. TRAINING\n",
        "print(\"\\n--- 2. TRAINING MLP ---\")\n",
        "train_ds = MemmapDataset(X_train_mmap, Y_dense, idx_train)\n",
        "val_ds   = MemmapDataset(X_train_mmap, Y_dense, idx_val)\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_ds,\n",
        "    batch_size=BATCH_SIZE_TRAIN,\n",
        "    shuffle=True,\n",
        "    pin_memory=True,\n",
        "    num_workers=2,\n",
        ")\n",
        "val_loader = DataLoader(\n",
        "    val_ds,\n",
        "    batch_size=BATCH_SIZE_TRAIN,\n",
        "    shuffle=False,\n",
        "    num_workers=2,\n",
        ")\n",
        "\n",
        "clf = MLP(embed_dim, HIDDEN_DIM, n_labels).to(device)\n",
        "optimizer = torch.optim.Adam(clf.parameters(), lr=LR)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "best_f1 = 0.0\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    clf.train()\n",
        "    total_loss = 0.0\n",
        "\n",
        "    for x, y in train_loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        logits = clf(x)\n",
        "        loss = criterion(logits, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    clf.eval()\n",
        "    preds_list, targets_list = [], []\n",
        "    with torch.no_grad():\n",
        "        for x, y in val_loader:\n",
        "            x = x.to(device)\n",
        "            p = torch.sigmoid(clf(x))\n",
        "            preds_list.append((p > 0.3).float().cpu().numpy())\n",
        "            targets_list.append(y.numpy())\n",
        "\n",
        "    f1 = f1_score(\n",
        "        np.vstack(targets_list),\n",
        "        np.vstack(preds_list),\n",
        "        average=\"micro\"\n",
        "    )\n",
        "    print(\n",
        "        f\"Epoch {epoch+1} | \"\n",
        "        f\"Loss: {total_loss/len(train_loader):.4f} | \"\n",
        "        f\"Val F1: {f1:.4f}\"\n",
        "    )\n",
        "\n",
        "    if f1 > best_f1:\n",
        "        best_f1 = f1\n",
        "        torch.save(clf.state_dict(), \"best_model.pth\")\n",
        "\n",
        "print(f\"Best Val F1: {best_f1:.4f}\")\n",
        "\n",
        "del train_ds, val_ds, train_loader, val_loader, Y_dense\n",
        "gc.collect()\n",
        "\n",
        "# 9. PREDICT TEST\n",
        "print(\"\\n--- 3. PREDICTING TEST DATA ---\")\n",
        "clf.load_state_dict(torch.load(\"best_model.pth\"))\n",
        "clf.eval()\n",
        "\n",
        "test_df = read_fasta(TEST_SEQ_PATH)\n",
        "\n",
        "# Cháº¡y Embedding Test\n",
        "X_test_mmap = get_memmap_embedding(\n",
        "    test_df[\"EntryID\"].tolist(),\n",
        "    test_df[\"sequence\"].tolist(),\n",
        "    EMBED_TEST_PATH,\n",
        "    batch_size=EMBED_BATCH_SIZE,\n",
        ")\n",
        "\n",
        "class TestDataset(Dataset):\n",
        "    def __init__(self, memmap_array):\n",
        "        self.memmap_array = memmap_array\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memmap_array)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return torch.tensor(self.memmap_array[idx], dtype=torch.float32)\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    TestDataset(X_test_mmap),\n",
        "    batch_size=BATCH_SIZE_TRAIN * 2,\n",
        "    shuffle=False,\n",
        "    num_workers=2,\n",
        ")\n",
        "\n",
        "all_probs = []\n",
        "with torch.no_grad():\n",
        "    for x in tqdm(test_loader, desc=\"Predicting\"):\n",
        "        all_probs.append(\n",
        "            torch.sigmoid(clf(x.to(device))).cpu().numpy()\n",
        "        )\n",
        "\n",
        "final_probs = np.vstack(all_probs)\n",
        "\n",
        "# 10. SUBMISSION\n",
        "print(\"\\n--- 4. CREATING SUBMISSION (OPTIMIZED) ---\")\n",
        "\n",
        "print(\"Building prediction DataFrame...\")\n",
        "test_ids = test_df[\"EntryID\"].tolist()\n",
        "pred_rows = []\n",
        "\n",
        "# Láº¥y Top 50 Ä‘iá»ƒm cao nháº¥t cho má»—i protein Ä‘á»ƒ giáº£m dung lÆ°á»£ng\n",
        "top_k = 50\n",
        "\n",
        "for idx, entry_id in enumerate(tqdm(test_ids, desc=\"Filtering Top Scores\")):\n",
        "    scores = final_probs[idx]\n",
        "\n",
        "    # Láº¥y index cá»§a top K Ä‘iá»ƒm cao nháº¥t\n",
        "    if len(scores) > top_k:\n",
        "        top_indices = np.argpartition(scores, -top_k)[-top_k:]\n",
        "    else:\n",
        "        top_indices = np.arange(len(scores))\n",
        "\n",
        "    for term_idx in top_indices:\n",
        "        score = scores[term_idx]\n",
        "        if score > 0.001:\n",
        "            pred_rows.append(\n",
        "                {\n",
        "                    \"EntryID\": entry_id,\n",
        "                    \"term\": mlb.classes_[term_idx],\n",
        "                    \"score\": float(score),\n",
        "                }\n",
        "            )\n",
        "\n",
        "my_preds_df = pd.DataFrame(pred_rows)\n",
        "\n",
        "# BÆ°á»›c 2: Load Sample Submission\n",
        "if os.path.exists(SAMPLE_SUB_PATH):\n",
        "    print(f\"Found sample submission at: {SAMPLE_SUB_PATH}\")\n",
        "    try:\n",
        "        # Äá»c thá»­ 1 dÃ²ng Ä‘á»ƒ check cá»™t trÆ°á»›c khi dÃ¹ng usecols\n",
        "        header_check = pd.read_csv(SAMPLE_SUB_PATH, sep=\"\\t\", nrows=1)\n",
        "        print(f\"   -> Columns detected: {header_check.columns.tolist()}\")\n",
        "\n",
        "        if \"EntryID\" in header_check.columns and \"term\" in header_check.columns:\n",
        "            print(\"   -> Template format looks correct. merging...\")\n",
        "            sample_df = pd.read_csv(\n",
        "                SAMPLE_SUB_PATH,\n",
        "                sep=\"\\t\",\n",
        "                usecols=[\"EntryID\", \"term\"],\n",
        "            )\n",
        "\n",
        "            # GhÃ©p Ä‘iá»ƒm dá»± Ä‘oÃ¡n vÃ o báº£ng máº«u (Left Join)\n",
        "            submission = sample_df.merge(\n",
        "                my_preds_df,\n",
        "                on=[\"EntryID\", \"term\"],\n",
        "                how=\"left\",\n",
        "            )\n",
        "            submission[\"score\"] = submission[\"score\"].fillna(0.001)\n",
        "        else:\n",
        "            print(\"   âš ï¸ WARNING: Sample file columns do not match ['EntryID', 'term']. Skipping merge.\")\n",
        "            print(\"   -> Using raw predictions instead.\")\n",
        "            submission = my_preds_df\n",
        "    except Exception as e:\n",
        "        print(f\"   âš ï¸ ERROR reading sample submission: {e}\")\n",
        "        print(\"   -> Using raw predictions instead.\")\n",
        "        submission = my_preds_df\n",
        "else:\n",
        "    print(\"Sample submission not found. Exporting all valid predictions...\")\n",
        "    submission = my_preds_df\n",
        "\n",
        "# Format láº¡i Ä‘iá»ƒm sá»‘\n",
        "submission[\"score\"] = submission[\"score\"].map(lambda x: f\"{x:.3f}\")\n",
        "\n",
        "output_path = \"/content/submission_esm2_optimized.tsv\"\n",
        "submission.to_csv(output_path, sep=\"\\t\", index=False)\n",
        "print(f\"âœ… Xong! File lÆ°u táº¡i: {output_path}\")\n",
        "\n",
        "try:\n",
        "    from google.colab import files\n",
        "    files.download(output_path)\n",
        "except Exception:\n",
        "    pass\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
